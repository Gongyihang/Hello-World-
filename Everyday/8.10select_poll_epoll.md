## 3.I/O复用技术select/poll/epoll区别，问select为啥限制1024
C10K问题：  
随着互联网的普及，应用的用户群体几何倍增长，此时服务器性能问题就出现。最初的服务器是基于进程/线程模型。新到来一个TCP连接，就需要分配一个进程。假如有C10K，就需要创建1W个进程，可想而知单机是无法承受的。那么如何突破单机性能是高性能网络编程必须要面对的问题，进而这些局限和问题就统称为C10K问题，最早是由Dan Kegel进行归纳和总结的，并且他也系统的分析和提出解决方案。  
本质：C10K问题的本质上是操作系统的问题。对于Web 1.0/2.0时代的操作系统，传统的同步阻塞I/O模型处理方式都是requests per second。当创建的进程或线程多了，数据拷贝频繁（缓存I/O、内核将数据拷贝到用户进程空间、阻塞，进程/线程上下文切换消耗大， 导致操作系统崩溃，这就是C10K问题的本质。  

可见, 解决C10K问题的关键就是尽可能减少这些CPU资源消耗。  

C10K问题的解决方案
从网络编程技术的角度来说，主要思路：  
1.每个连接分配一个独立的线程/进程  

2.同一个线程/进程同时处理多个连接  

#### 每个进程/线程处理一个连接  

该思路最为直接，但是申请进程/线程是需要系统资源的，且系统需要管理这些进程/线程，所以会使资源占用过多，可扩展性差  

#### 每个进程/线程同时处理 多个连接(I/O多路复用)  
[一文读懂I/O多路复用技术](https://www.jianshu.com/p/9cadb8b358d7)  

select方式：使用fd_set结构体告诉内核同时监控那些文件句柄，使用逐个排查方式去检查是否有文件句柄就绪或者超时。  
该方式有以下缺点：文件句柄数量是有上线的，逐个检查吞吐量低，每次调用都要重复初始化fd_set。  
poll方式：该方式主要解决了select方式的2个缺点，文件句柄上限问题(链表方式存储)以及重复初始化问题(不同字段标注关注事件和发生事件)  
但是逐个去检查文件句柄是否就绪的问题仍然没有解决。  
epoll方式：该方式可以说是C10K问题的killer，他不去轮询监听所有文件句柄是否已经就绪。epoll只对发生变化的文件句柄感兴趣。  
其工作机制是，使用"事件"的就绪通知方式，通过epoll_ctl注册文件描述符fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd, epoll_wait便可以收到通知, 并通知应用程序。  
而且epoll使用一个文件描述符管理多个描述符,将用户进程的文件描述符的事件存放到内核的一个事件表中, 这样数据只需要从内核缓存空间拷贝一次到用户进程地址空间。  
而且epoll是通过内核与用户空间共享内存方式来实现事件就绪消息传递的，其效率非常高。但是epoll是依赖系统的(Linux)。  
异步I/O以及Windows，该方式在windows上支持很好，这里就不具体介绍啦。

简单说一下BIO，说一下它的缺点：
BIO中的B，表示的是blocking的意思，就是阻塞。作为服务端开发，我们使用server socket绑定完端口号之后，我们会监听该端口，等待accept事件，accept会阻塞当前主线程，当我们收到accept事件时，程序就会拿到一个客户端与当前服务端连接的socket，针对这个socket我们可以进行读写，但是呢，这个socket读写方法都是会阻塞当前线程的，一般我们会使用多线程的方式来进行c/s交互，但是这样就很难解决C10K问题(比如说1w个客户端就需要1w个线程去支持，这样的话且不说cpu肯定会爆炸了，线程上下文切换，也会把机器负载给拉飞了)  

NIO靠什么解决C10K？  
比如我们调用NOI的api，它提供一套非阻塞的接口，这样就不需要我们为每一个c/s长连接保留一个单独的处理线程了，阻塞IO之所以需要socket长连接，指定一个线程，就是因为它阻塞，现在这个NIO API具备非阻塞特性了，就可以用一个线程去检查n个socket，比如nio包提供了一个选择器selector，我们把检查的socket注册到这个selector中，然后主线程阻塞在selector#select方法里，当选择器发现某个socket就绪了，就会唤醒主线程，然后咱们可以通过selector获取到就绪状态的socket进行相应的处理。  

操作系统kernel提供的select函数实现原理：
每次调用kernel#select函数，它都会涉及到用户态/内核态的切换，还需要传递需要检查的socket集合，其实就是检查fd（文件描述符id），因为程序都是运行在linux或者unix操作系统上，这种操作系统上，一切皆文件，socket也不例外，这里传递的fd其实就是文件系统中对应socket生成的文件描述符，socket函数被调用以后，首先会按照fd集合，去检查内存中socket套接字状态，这个复杂度是O（N）的，检查完一遍之后，如果有就绪状态的socket，那么直接返回，不会阻塞当前调用线程。否则，就说明当前指定fd集合对应的socket没有就绪状态的，那么就需要阻塞当前调用线程了，直到有某个socket有数据之后，才唤醒线程。
select函数去监听socket的时候，socket数量有没有什么限制？  
默认最大可以监听1024个socket（实际要小于1024）
为什么？  
fd_set是select函数的参数之一，因为fd_set这个结构它是一个bitmap位图结构，
这个位图结构就是一个长的二进制数，类似于0101这种，这个bitmap默认长度是1024个bit，要想修改这个长度的话，非常麻烦，需要重新编译操作系统内核（针线活，一般人搞不定）。另一点，默认给的1024个bit，它是处于性能的考虑吧。因为select函数检查就绪状态的socket后，它做了两件事，第一件事跑到就绪状态的socket对应的filedescriptor（fd文件）中设置一个标记，标记一个mask，表示当前fd对应的socket就绪了，第二件事就是返回select函数，对应的也就是唤醒线程，返回一个int结果值，表示有几个socket处于就绪状态，具体的哪个socket就绪，java程序是不知道的。所以接下来又是一个O（N）的系统调用，检查fd，fd_set集合中每一个socket的就绪状态，其实就是检查文件系统中指定socket的文件描述符状态，涉及到用户态-内核态来回切换，如果bitmap再大，就需要更多的系统调用，开销就非常非常大了，系统调用涉及到参数的数据拷贝，如果数据太庞大，它也不利于系统调用的速度。  
假设select函数第一遍O（N）去检查时未发现就绪状态的socket，然后假设过了一会之后，有某一个socket它就绪了。那这个select函数它是怎么发现的呢？难道这个select函数它在底层kernel内一直是占着cpu去轮询去检查这些socket吗？  
操作系统调度和操作系统中断，先说调度，cpu（单核心）同一时刻，它只能运行一个进程，操作系统最主要的任务就是系统调度嘛，就是有n个进程，然后让n个进程在cpu上切换执行，未挂起的进程都在工作队列内，都有机会获取到cpu执行权，挂起的进程，就会从这个工作队列内移除出去，反映到程序层面就是线程阻塞了，linux系统线程其实就是轻量级的进程，然后咱们再说一下操作系统中断，这个非常重要，比如咱们用键盘打字，如果cpu正执行着其他程序，一直不释放，那咱们这个打字没法打了，但是咱们知道，不是这样的，因为有了系统中断的存在，我们按下一个键了以后，会给主板发送一个电流信号，主板感知到以后会触发cpu中断，所谓中断，其实就是让cpu正在执行的进程先保留程序上下文，然后避让出cpu，给中断程序让道，中断程序就会拿到cpu执行权限，进行相应代码执行，比如键盘的中断程序，他就会执行输出逻辑等等。  
咱在回到这个问题上，select函数，第一遍轮询，它没有发现就绪状态的socket，他就会把当前进程，保留给需要检查的socket的等待队列中，也就是这个socket结构，有三块核心区域，分别是读缓存，写缓存还有这个等待队列，这个select函数，它把当前进程保留到每个需要检查的socket#等待队列之后，就会把当前进程从工作队列移除了，移除之后，其实就是挂起当前进程了，select函数也就不会再运行了嘛，这个阶段完了，之后，然后咱们再说下一个阶段。假设咱们客户端往当前服务器发送了数据，数据通过网线到网卡，网卡再到DMA硬件的这种方式直接将数据写到内存里头，整个过程cpu是不参与的，当数据完成传输以后，它会触发网络数据传输完毕的中断程序了，这个中断程序它会把cpu正在执行的进程给顶掉，然后cpu就会执行咱这个中断程序的逻辑了。大概是这样，根据内存中有的数据包，然后分析出来数据包是哪个socket的数据，tcp/ip协议，它又保证传输的时候是有端口号的，数据包是有端口号的，然后根据端口号就能找到socket实例，找到socket实例以后，就把数据导入到socket的读缓冲区里头，导入完成以后，它就开始去检查socket的等待队列，是不是有等待者？如果有的话，咱就把等待者移动到工作队列，中断程序到这一步就执行完了，咱们的进程又回归到工作队列了，又有机会获取到cpu时间片了，然后这个当前进程执行select函数再检查，再次检查，就发现有就绪的socket了，它就会给就绪的socket的fd文件描述符打标记，然后select函数就执行完了，它返回到java程序层面，就涉及到内核态-用户态的转换，后面的事情就是轮询检查每一个socket的fd是否被打标记，然后处理被打标记的socket就ok了。

poll函数和select函数的区别  
最大区别就是传参不一样了，select使用的是bitmap，表示需要检查的socket集合。  
poll使用的是数组结构，表示需要检查的socket集合。主要就是为了解决bitmap长度是1024这个问题，poll使用数组就没有这个限制了，他就可以让咱们线程监听超过1024个socket限制。
epoll产生的背景是什么？  
epoll主要是为了解决select和poll函数的缺陷吧，我们先说一下select和poll他俩共有的缺陷。
第一个是这两个函数每次调用都需要我们提供给它所有的需要监听的socket文件描述符集合，而且咱们程序主线程是死循环调用select/poll函数的，这里面涉及到用户空间数据到内核态空间拷贝的过程，这个过程是比较耗费性能的，但是咱们监听socket集合，数据变化非常小，可能每次就1~2个socket_fd需要更改，但是没有办法，因为select和poll函数只是一个很单纯的函数，他在kernel层面，不会保留任何数据信息，所以说每次调用都进行数据拷贝了，这是第一个缺陷。  
第二个缺陷，select和poll函数的返回值是个int整型值，只能代表有几个socket就绪或者是有错误了，没办法表示出具体是哪一个socket就绪了，这就导致咱们程序被唤醒之后，它还需要新的一轮调用去检查哪个socket是就绪状态，然后在进行socket数据处理逻辑，在这里走了不少弯路。因为在系统调用需要涉及到用户态和内核态的来回切换，这个缺陷更严重了。  
主要缺陷就是这些，这也是epoll的产生背景，主要目的是为了解决这两个问题。  
epoll是如何设计的？  
为了提升效率就要解决这两个问题，第一个问题是函数调用参数拷贝问题，第二个是系统调用返回后不知道哪些socket就绪的问题，解决这两个问题，就需要epoll函数在内核空间内，它有一个对应的数据结构去存储一些数据，这个数据结构其实就是eventpoll对象，它可以通过一个系统函数epoll_create()去创建，创建完成之后，系统函数返回一个eventpoll对象的id，相当于我们在内核开辟了一小块空间，并且我们也知道这块空间的位置。eventpoll的结构主要是两块区域，其中一块是存放需要监听的socket_fd描述符列表，另一块区域是就绪列表，存放就绪状态的socket信息。另外还提供两个函数，一个是epoll_ctl函数，另外一个是epoll_wait函数。epoll_ctl它可以根据eventpoll_id区增删改内核空间上的eventpoll对象的检查列表（即关注的socket信息）。
去增加或者修改需要检查的socket文件描述符，epoll_wait()，它主要的参数是eventpoll_id表示此次系统调用需要检测的socket_fd集合，是eventpoll中已经定好的那些socket信息，epoll_wait函数，他默认情况下会阻塞调用线程，直到eventpoll中关联的某个或者某些个socket就绪以后，epoll_wait()才会返回。

eventpoll中药存放需要监视的socket集合信息，这个存放socket集合信息用的是什么数据结构？  
红黑树结构，因为socket集合信息经常会有增删改查的需求，这个需求，红黑树一定是最适合的了，他能保持一种相对稳定的查找效率，复杂度是O(logN)。  

epoll除了提供select/poll那种IO事件的电平触发 （Level Triggered）外，还提供了边沿触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少epoll_wait/epoll_pwait的调用，提高应用程序效率。Linux2.6内核中对/dev/epoll设备的访问的封装（system epoll）。这个使我们开发网络应用程序更加简单，并且更加高效。为什么我们对epoll情有独钟呢？原因如下：1.文件描述符数量的对比。epoll并没有fd(文件描述符)的上限，它只跟系统内存有关，我的2G的ubuntu下查看是20480个，轻松支持20W个fd。可使用如下命令查看：cat /proc/sys/fs/file-max再来看select/poll，有一个限定的fd的数量，linux/posix_types.h头文件中#define __FD_SETSIZE    10242.效率对比。当然了，你可以修改上述值，然后重新编译内核，然后再次写代码，这也是没问题的，不过我先说说select/poll的机制，估计你马上会作废上面修改枚举值的想法。select/poll会因为监听fd的数量而导致效率低下，因为它是轮询所有fd，有数据就处理，没数据就跳过，所以fd的数量会降低效率；而epoll只处理就绪的fd，它有一个就绪设备的队列，每次只轮询该队列的数据，然后进行处理。(先简单讲一下，第二篇还会详细讲解)3.内存处理方式对比。不管是哪种I/O机制，都无法避免fd在操作过程中拷贝的问题，而epoll使用了mmap(是指文件/对象的内存映射，被映射到多个内存页上)，所以同一块内存就可以避免这个问题。btw:TCP/IP协议栈使用内存池管理sk_buff结构，你还可以通过修改内存池pool的大小，毕竟linux支持各种微调内核。  


epoll水平触发: 只要监听的文件描述符中有数据，就会触发epoll_wait有返回值，这是默认的epoll_wait的方式;
![水平触发](https://pic2.zhimg.com/v2-e71379bea286dce3b18dd110576f7b58_b.jpg)

epoll边沿触发 : 只有监听的文件描述符的读/写事件发生，才会触发epoll_wait有返回值;  
![边沿触发](https://pic3.zhimg.com/v2-1032b7f7926d19104fcd8ae3c299921b_b.jpg)